Agent Teams Research (latest only)
Date: 2026-02-08
Scope: Agent Teams only. Subagents docs intentionally excluded.

Official sources used
1) Agent Teams docs (core reference):
   https://code.claude.com/docs/en/agent-teams
2) Claude Code changelog (release-note signal for Agent Teams feature):
   https://raw.githubusercontent.com/anthropics/claude-code/main/CHANGELOG.md
3) Opus 4.6 announcement page (date context):
   https://www.anthropic.com/claude/opus
4) Anthropic engineering post using Agent Teams (date context + behavior evidence):
   https://www.anthropic.com/engineering/building-c-compiler

Release-date context (official)
- Claude Opus 4.6 announcement date is Feb 5, 2026 on Anthropicâ€™s Opus page.
- Claude Code changelog entry 2.1.32 adds "research preview agent teams" and references the enable flag.
- This matches your requested window around Feb 4-5, 2026.

What Agent Teams provides (from the Agent Teams docs)
- Multi-session orchestration with a lead plus independent teammates.
- Shared task list with dependency handling and file-lock task claiming.
- Direct teammate-to-teammate and lead-to-teammate communication.
- Delegate mode so the lead can stay coordination-only.
- Plan-approval flow for risky tasks before implementation.
- Hook-based quality gates (e.g., TeammateIdle / TaskCompleted).
- Explicit warning that Agent Teams are token-intensive and have coordination overhead.

How we should outperform Agent Teams in this project
1) Keep quality equal-or-better with hard gates in CI.
2) Keep usage lower via adaptive fanout/rebalancing instead of fixed high parallelism.
3) Enforce deterministic safety/reliability with permission profiles, hooks, leases, and orphan recovery.
4) Use checkpoint compaction/context reset to reduce persistent context bloat.

Current proof in this repo
- Internal quality-vs-usage benchmark report:
  docs/benchmark-report-v2-2026-02-08.md
- Raw report artifact:
  .tmp/v2-claude-advantage-report.json
- Latest measured result: quality unchanged (1.0 median) with lower median tokens (-1322.5) vs fixed-6 baseline.

Important framing
- This benchmark is internal (adaptive vs fixed baseline) and proves better efficiency at equal quality in our orchestrator.
- A strict external "better than Claude Agent Teams" claim requires side-by-side A/B runs on identical task sets against Claude Agent Teams directly.
